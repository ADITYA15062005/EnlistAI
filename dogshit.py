import os
import requests
import torch
import zipfile
import streamlit as st
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# ğŸ”’ Load environment variables
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# ğŸ“Œ Streamlit UI setup
st.set_page_config(page_title="EnlistAI - Interview Prep", page_icon="ğŸ¯")
st.title("ğŸ¯ EnlistAI â€“ Company Insights & Interview Prep")
st.markdown("---")

# ğŸ§  Load embedding model
device = "cuda" if torch.cuda.is_available() else "cpu"
embedder = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": device}
)
st.success("ğŸ” Embedding model loaded")

# ğŸ“ Unzip and Load FAISS index
zip_path = "faiss_index_interview_questions.zip"
faiss_dir = "faiss_index_interview_questions"

try:
    if not os.path.exists(faiss_dir):
        with zipfile.ZipFile(zip_path, "r") as zip_ref:
            zip_ref.extractall(faiss_dir)
        st.info("ğŸ—ƒï¸ FAISS index unzipped.")

    db = FAISS.load_local(
        folder_path=faiss_dir,
        embeddings=embedder,
        allow_dangerous_deserialization=True
    )
    retriever = db.as_retriever(search_kwargs={"k": 3})
    st.success("ğŸ“ FAISS index loaded")
except Exception as e:
    st.error(f"âŒ Failed to load FAISS index: {e}")

# ğŸ¤– Connect to Groq LLM
llm = ChatGroq(api_key=GROQ_API_KEY, model="llama3-70b-8192")
st.success("ğŸ¤– Groq LLM connected")

# ğŸ“œ Prompt template
template = """
You are a company insights assistant.

Here is what we know:

Company Details:
{company_info}

Interview Questions:
{interview_questions}

Now, answer this user question clearly and professionally:
{user_question}
"""

prompt = PromptTemplate(
    input_variables=["company_info", "interview_questions", "user_question"],
    template=template
)

# â›“ï¸ Chain: prompt | llm
chain = prompt | llm

# ğŸ¢ UI Inputs
company_name = st.text_input("ğŸ¢ Enter Company Name")
user_question = st.text_input("â“ Ask your question about the company")

# ğŸš€ Main Logic
if company_name and user_question:
    try:
        # 1ï¸âƒ£ Get company details
        info_url = f"https://enlistai.onrender.com/company/{company_name}"
        info = requests.get(info_url).json()
        company_info = "\n".join(f"{k}: {v}" for k, v in info.items())

        # 2ï¸âƒ£ Get interview questions
        questions_url = f"https://enlistai-interview-question.onrender.com/questions/{company_name}"
        q_res = requests.get(questions_url).json()
        if isinstance(q_res, list):
            interview_questions = "\n".join(q_res)
        elif isinstance(q_res, dict):
            interview_questions = "\n".join(q_res.get("Questions", []))
        else:
            interview_questions = "No interview questions found."

        # 3ï¸âƒ£ Call chain
        with st.spinner("ğŸ¤– Thinking..."):
            answer = chain.invoke({
                "company_info": company_info,
                "interview_questions": interview_questions,
                "user_question": user_question
            })

        # 4ï¸âƒ£ Display
        st.markdown("### âœ… Answer")
        st.write(answer.content if hasattr(answer, "content") else answer)

    except Exception as e:
        st.error(f"âš ï¸ Error occurred: {e}")
