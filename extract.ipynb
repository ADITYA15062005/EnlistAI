{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c858cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd728373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012fceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "\n",
      "✅ Scraping complete! Saved 500 companies to top_companies.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def scrape_ambitionbox_top_companies(pages=25, output_file=\"top_companies.csv\"):\n",
    "    options = Options()\n",
    "    # Comment this line if you want to see the browser window\n",
    "    # options.add_argument(\"--headless\")\n",
    "\n",
    "    # Anti-bot headers\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.110 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    all_data = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"https://www.ambitionbox.com/list-of-companies?page={page}\"\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        driver.get(url)\n",
    "        time.sleep(6)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        cards = soup.find_all(\"div\", class_=\"companyCardWrapper\")\n",
    "\n",
    "        if not cards:\n",
    "            print(\"⚠️ No companies found on page. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for card in cards:\n",
    "            name = card.find(\"meta\", {\"itemprop\": \"name\"})\n",
    "            name = name.get(\"content\", \"N/A\") if name else \"N/A\"\n",
    "\n",
    "            rating_tag = card.find(\"div\", class_=\"rating_text\")\n",
    "            rating = rating_tag.text.strip() if rating_tag else \"N/A\"\n",
    "\n",
    "            review_tag = card.find(\"span\", class_=\"companyCardWrapper__companyRatingCount\")\n",
    "            reviews = review_tag.text.strip() if review_tag else \"N/A\"\n",
    "\n",
    "            location_tag = card.find(\"span\", class_=\"companyCardWrapper__interLinking\")\n",
    "            location = location_tag.text.strip() if location_tag else \"N/A\"\n",
    "\n",
    "            all_data.append({\n",
    "                \"Company\": name,\n",
    "                \"Rating\": rating,\n",
    "                \"Reviews\": reviews,\n",
    "                \"Industry & Location\": location\n",
    "            })\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save to CSV\n",
    "    keys = [\"Company\", \"Rating\", \"Reviews\", \"Industry & Location\"]\n",
    "    with open(output_file, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "\n",
    "    print(f\"\\nScraping complete! Saved {len(all_data)} companies to {output_file}\")\n",
    "\n",
    "# Run it\n",
    "scrape_ambitionbox_top_companies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d42910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\adity\\Desktop\\gemma\\top_companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf37c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Industry &amp; Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCS</td>\n",
       "      <td>3.6</td>\n",
       "      <td>(98.1k)</td>\n",
       "      <td>IT Services &amp; Consulting | Bangalore / Bengalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>3.8</td>\n",
       "      <td>(63.3k)</td>\n",
       "      <td>IT Services &amp; Consulting | Bangalore / Bengalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>3.7</td>\n",
       "      <td>(57.3k)</td>\n",
       "      <td>IT Services &amp; Consulting | Bangalore / Bengalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cognizant</td>\n",
       "      <td>3.7</td>\n",
       "      <td>(54.4k)</td>\n",
       "      <td>IT Services &amp; Consulting | Hyderabad / Secunde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capgemini</td>\n",
       "      <td>3.7</td>\n",
       "      <td>(46.1k)</td>\n",
       "      <td>IT Services &amp; Consulting | Bangalore / Bengalu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Company  Rating  Reviews  \\\n",
       "0        TCS     3.6  (98.1k)   \n",
       "1  Accenture     3.8  (63.3k)   \n",
       "2      Wipro     3.7  (57.3k)   \n",
       "3  Cognizant     3.7  (54.4k)   \n",
       "4  Capgemini     3.7  (46.1k)   \n",
       "\n",
       "                                 Industry & Location  \n",
       "0  IT Services & Consulting | Bangalore / Bengalu...  \n",
       "1  IT Services & Consulting | Bangalore / Bengalu...  \n",
       "2  IT Services & Consulting | Bangalore / Bengalu...  \n",
       "3  IT Services & Consulting | Hyderabad / Secunde...  \n",
       "4  IT Services & Consulting | Bangalore / Bengalu...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. How proficient are you in Python?\n",
      "2. How would you restructure your current CTC?\n",
      "3. What is IPL in Mainframe?\n",
      "4. What is Salesforce?\n",
      "5. What are the steps to create and handle a custom exception in programming?\n",
      "6. What annotations are used in a REST API?\n",
      "7. What is the difference between the '==' operator and the '.equals()' method in Java?\n",
      "8. What does error code 57 signify?\n",
      "9. What does AVRD mean?\n",
      "10. What are some effective strategies for stress management?\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "def slugify(name):\n",
    "    slug = name.lower()\n",
    "    slug = re.sub(r'[^a-z0-9\\s-]', '', slug)\n",
    "    slug = re.sub(r'\\s+', '-', slug.strip())\n",
    "    return slug\n",
    "\n",
    "def get_interview_questions(company_name, max_questions=20):\n",
    "    slug = slugify(company_name)\n",
    "    url = f\"https://www.ambitionbox.com/interviews/{slug}-interview-questions\"\n",
    "\n",
    "    options = Options()\n",
    "    # Use visible browser to verify it works\n",
    "    # comment next line to see browser\n",
    "    # options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(6)\n",
    "\n",
    "        # Scroll to load more\n",
    "        for _ in range(5):\n",
    "            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return []\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    results = []\n",
    "    question_tags = soup.find_all(\"div\", class_=\"topQuesAnsCard__details--question\")\n",
    "\n",
    "    for tag in question_tags:\n",
    "        if len(results) >= max_questions:\n",
    "            break\n",
    "        span = tag.find(\"span\", class_=\"topQuesAnsCard__details--answerHtmlWrapper\")\n",
    "        if span:\n",
    "            q = span.get_text(strip=True)\n",
    "            q = q.replace(\"Q.\", \"\").strip()\n",
    "            results.append((company_name, q))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test\n",
    "questions = get_interview_questions(\"TCS\")\n",
    "\n",
    "for i, (company, q) in enumerate(questions, 1):\n",
    "    print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a17d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Scraping: TCS\n",
      "[2/5] Scraping: Accenture\n",
      "[3/5] Scraping: Wipro\n",
      "[4/5] Scraping: Infosys\n",
      "[5/5] Scraping: Cognizant\n",
      "✅ Done. Data saved to interview_questions_500_companies.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def slugify(name):\n",
    "    slug = name.lower()\n",
    "    slug = re.sub(r'[^a-z0-9\\s-]', '', slug)\n",
    "    slug = re.sub(r'\\s+', '-', slug.strip())\n",
    "    return slug\n",
    "\n",
    "def get_interview_questions(company_name, max_questions=20):\n",
    "    slug = slugify(company_name)\n",
    "    url = f\"https://www.ambitionbox.com/interviews/{slug}-interview-questions\"\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(6)\n",
    "        for _ in range(5):\n",
    "            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "            time.sleep(1.5)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return []\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    results = []\n",
    "    question_tags = soup.find_all(\"div\", class_=\"topQuesAnsCard__details--question\")\n",
    "\n",
    "    for tag in question_tags:\n",
    "        if len(results) >= max_questions:\n",
    "            break\n",
    "        span = tag.find(\"span\", class_=\"topQuesAnsCard__details--answerHtmlWrapper\")\n",
    "        if span:\n",
    "            q = span.get_text(strip=True).replace(\"Q.\", \"\").strip()\n",
    "            results.append((company_name, q))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example company list (replace with all 500)\n",
    "top_companies = [\"TCS\", \"Accenture\", \"Wipro\", \"Infosys\", \"Cognizant\"]\n",
    "\n",
    "# Output file\n",
    "with open(\"interview_questions_500_companies.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Company\", \"Question\"])\n",
    "\n",
    "    for idx, company in enumerate(top_companies, 1):\n",
    "        print(f\"[{idx}/{len(top_companies)}] Scraping: {company}\")\n",
    "        questions = get_interview_questions(company)\n",
    "        if not questions:\n",
    "            print(f\"No questions found for {company}\")\n",
    "        for row in questions:\n",
    "            writer.writerow(row)\n",
    "        time.sleep(2)\n",
    "\n",
    "print(\"Done. Data saved to interview_questions_500_companies.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4245c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
